# 数据相关

## 数据获取
开源数据获取方式：
- OpenDataLab
- 上海浦源
- BAAI北京智源研究生
- Huggingface
- Modelscope
 
## 数据优化

### 如何处理连续型数据？
- normalization：
  - 将数据压缩到0,1区间。使得神经网络不会因为输入数据大小太大而导致网络梯度一下子太大或者太小而导致的梯度弥散或者爆炸。
- standardization：
  - 将数据分布调整为正态分布
    - 可以加快模型收敛速度：原因是这样的
    - 首先，因为我们常用的优化器是adam，他是基于梯度下降算法的。然而梯度下降算法对于数据尺度（分布）较为铭感。
    - 当数据的尺度不一致时，优化器可能会在不同的方向上表现出不同的收敛速度。通过标准化，可以确保不同特征之间的尺度相似，从而使优化器更容易找到最优解，以此加快收敛速度。
  - 尤其是当不知道数据分布或者数据分布呈现正态分布的趋势时

### （数据预处理）什么时候不能用standardization和normalization？
对于在数据预处理normalization，数据存在极端值：
- 他对极端值很铭感
- 在进行normalization后，数据的不再是原本的真实分布，而是容易靠向极端数据位置。
- 举个例子：例如最大值是100w，但是别的数据是0-100
- 解决方法：
  - 使用中位数，4分位数，8分位数解决

对于在数据预处理standardization，有数据为0的情况：
- 可能在最初，这个0意味着被补充的（padding的作用），是没意义，类似于None的作用
- 但是当我们使用标准化时，本来没有意义的东西0，变得有意义，从而导致无中生有！

### 如何处理离散数据？
- 独热编码one-hot
- 稠密编码（embedding）：
  - embedding通过训练而得，用n个向量表述一个词。

## 数据采集标准：怎么样才算高质量数据？（衡量数据的标准
- 数据量：足够的数据帮助模型学习到更加全面的特征，不足会过拟合。
- 多样性：意味着数据包含了广泛的情况，帮助模型识别不同场景下的情况，从而提高模型的泛化性。
- 均衡性：数据均衡性意味着数据中各种类别的数据量是否相对平衡，如果某个数据量特别多某个特别少，这会导致训练后的模型在使用时（预测时）会更加偏向数据量多的那个类别。
- 覆盖性：意味着讲所有数据类别都涵盖，比如总数据10个分类，但是只采样了9类别
- 一致性：同样一个问题你打标签是0，他打标签为1，那么一致性差
- IID，独立同分布：从数学层面来说代表了两个关键特性：独立性和同分布。
  - 前者代表了每个随机变量互相独立。对于后者，代表了这些随机变量来源于同一个概率分布。
  - 在机器学习和深度学习中，IID假设通常应用于训练数据集，意味着：
    - 每个训练样本都是独立地从相同的数据生成过程中抽取的。
    - 无论是训练集还是测试集，数据点都来自于同一个潜在的概率分布。
  - 这个假设简化了模型的设计和分析，因为它允许我们使用基于统计的方法来估计参数，并且可以预期模型在新数据上会有类似的表现。

## 数据清洗：有哪些手段？
- 去除异常值
- 去重
- 去除极端值
- 补全缺失值
- 保证一致性

## 数据增强：手段有哪些？
- 简单增强：通过简单的换算就增强
  - 线性增强：
    - 例如放大缩小，镜像，调整明暗
    - 对模型增强的效果有限，因为模型很快就学会了
- 复杂的增强：
  - 盐胡椒噪音，高斯噪声，等噪声
  - 马赛克技术（dropblock）：掩码然后猜图像是什么
    - 效果不错，因为其实里面包含了预测性任务
  - 拼接：将几个图/数据拼在一起
- 生成增强：用别的模型来生成数据，用的比较少

## 如何解决数据不平衡问题？
解决方案：
  - 重采样
    - 过采样：针对小数量类型的数据增加其数据量
    - 降采样：针对大数量类型的数据减少其数据量
  - 迁移学习
什么是不平衡数据集：
  - 不平衡数据集指的是数据集各个类别的样本数目相差巨大。以二分类问题为例，假设正类的样本数量远大于负类的样本数量。
不平衡数据带来的问题：
- 如果不均衡比例超过4:1，分类器就会偏向于大的类别，在这种情况下，该分类器是无效的，尽管最后的分类准确度为90%。因此这个时候准确率（accuracy）作为评价指标也是不靠谱的。

## 非常重要要！
### 对于大模型数据怎么做？
数据被提供：
- 数据预处理

数据没被提供：
- 自己公司数据，开源数据，大模型生成的数据

如果数据特别少很难做，很稀缺：
- 先迁移学习别的数据

最后都得：
- 训练数据要shuffle，跟换数据集的时候需要将之前的数据和现在更换的数据做个配比，之前数据20%现在数据80%（这个指标只是我自己大概一下）
  - 在这种情况下，可以确保模型训练时不会因为看到自己陌生的数据（不同分布的数据）而导致loss上升比较多
  - 从而减少模型训练不稳定的情况出现。

# 模型优化

## 拟合问题

### 什么是过拟合和欠拟合？
过拟合：
- 数学的角度来说：模型的参数量远远大于数据的信息量，并且对w的解空间特别大。
- 通识来说：就是在特定数据上表现结果非常好，但是当换一个场景时，表现非常差，没有很好的泛化性。
- 明显表现：高训练准确率，低测试准确率。模型可能生成不具通用性的输出，只能在训练数据的上下文中合理。

欠拟合：模型在训练数据和测试数据上都表现不佳。
- 明显表现：训练和测试准确率都低。模型可能生成的输出太过普通或无关，无法满足特定任务的需求

### 什么原因导致过拟合和欠拟合，怎么解决这些问题？
过拟合问题:（原因+Solution）
- 模型太聪明了
  - 优化模型网络结构或者参数（减少模型层数，使用瓶颈结构）
  - 正则化：对模型添加惩罚项，限制其复杂度（增加拟合曲线的宽度）
    - L2正则
    - 权重衰减
  - Dropout：随机断开某些神经元的连接
- 数据集数据量太小了
  - 增加采集数据量：增加数据集的多样性
  - 数据增强：
    - 简单的增强（线性增强）：
      - 平移
      - 旋转
      - 缩放
      - 错切
      - 镜像
    - 复杂增强：
      - 颜色，饱和度，明暗
      - 噪音：高斯噪音，椒盐噪音
  - 迁移学习：首先在相似任务的较多的数据集上进行微调（以一种预训练的感觉去微调它），然后在现在这个小数据集上微调，以便模型获得更好的初始参数。

欠拟合问题：
- 模型太简单
  - 加深网络层数，扩大网络规模和参数量
- 数据集质量太差
  - 数据清洗：去除无关样本和噪音，但是保留一定噪音（特别是图像任务中，因为只有这样可以更好的增加模型的泛化性，但又保证模型不会欠拟合）
- 不合适的优化算法和损失函数
  - 使用adam优化器：确保模型不会因为使用梯度下降法训练，而模型陷入局部最优造成的欠拟合问题
- 训练时间不足
  - 增加训练轮次 

### 什么是正则化？
把参数压到一个很小的空间，相当于减少了参数的搜索空间。 

正则化加载在损失函数上的。    
分为主要部分和次要部分：    
- 主要部分就是原始的模型的损失函数
- 次要部分就是添加到损失函数上的正则化部分
  - 正则化系数越大，模型受到的影响越多
  - L2常用，使用模型参数的二范数作为正则化，也就是欧氏距离
    - 把不重要的权重缩小，重要的放大
  - L1，是采用1范数，也就是曼哈顿距离
    - 把不重要的权重变为0，重要的放大
- 作用：    
  - 降低梯度爆炸的风险（约束模型的梯度大小，使得权重不会变得过大）
  - 减小模型复杂度
  - 由于复杂度降低，也就降低了过拟合风险


## 梯度问题

### 梯度消失的解决方法（记得回答的时候要先说原因再说解决方法：
- 梯度弥散和爆炸的根本原因：
  - 梯度在每层传递过程中乘以一个小/大于 1 的值，导致梯度逐渐变小/大，最终趋近于零/无穷。

- 选择合适的激活函数，避免选择sigmoid，tanh（有饱和区,导数/梯度为0），尤其是当网络层数较深的时候。
  - 可以选择leakyRelu，但是swish更加好，不要ReLu（因为有死区）
  
- 减小模型层数
  
- 初始化权重：
  - 原因：不初始化权重，意味着权重可能很大可能很小，网络反向传播的时候计算损失可能非常大小，梯度更新也可能非常大/小，由于链式法则，梯度弥散或者爆炸。
  - 一般我使用xavier均匀初始化进行权重初始化，他们会是线性层和embeding层得初始化。但如果是对激活函数初始化，我会选择kaiming均匀初始化，也就是he初始化，因为它适用于
relu及其变体激活函数。
  - 为什么不初始化为正态分布？
    - 因为正态分布的权重分布意味着权重有重要程度的区别，但是在模型最开始训练的时候，我们应该对所有网络节点一视同仁，因此采用均匀初始化
  - 为什么不初始化为同一数值？
    - 因为同一数值意味着，对于每一个权重，他们权重更新的变化量是一样的，那么也就是说每个神经元节点都是一样权重，这样学习是没有意义的。
  - 为什么不初始化为随机权重呢？
    - 毕竟权重如果初始化的数值太大或者太小，在链式法则的加持下，梯度也会要么很大要么很小，随着网络加深梯度弥散或者爆炸这样的问题就出现了。
  
- 数据处理中Normalization：
  - 对于连续性数据，将数据压缩到0,1区间。使得神经网络不会因为输入数据大小太大而导致网络梯度一下子太大或者太小
  - 损失差距大/小，梯度更新非常大/小,由于链式法则
  - 而导致的梯度弥散或者爆炸。
  
- 神经网络中batch normalization等，对于大语言模型采用RMS normalization，之前是batch normalization，
  - 都是在数据输入之前，将数据分布强制为标准正态分布，也就是方差1和均值0。
    1.稳定的输入分布有助于保持梯度的大小在一个合理的范围内
    2.正态分布的性质使得输入数据的统计特性更加均匀，有助于保持信息的传递。
      - 如果输入数据的分布非常偏斜或集中在一个很小的范围内，可能会导致某些特征的信息丢失
    3.稳定的输入分布有助于优化器更好地调整学习率，从而稳定训练模型
  - 为什么不能用别的方式？
    - 为什么不能简单的进行数据缩放？
      - 将输入数据缩放到 [0, 1] 范围可能会导致某些特征的重要性被削弱，从而影响模型的性能。
    - 为什么不用均匀分布初始化？
      - 因为自然界很多都是正态分布数据
        - 因此输入数据的分布接近正态分布可以更好地模拟自然数据的特性，从而提高模型的泛化能力。
      - 均匀分布会将数据转换为固定范围，限制模型表达能力，丢失信息
        - 例如：如果输入数据的范围是 [-10, 10]，而均匀分布的范围是 [-1, 1]，直接将数据缩放到这个范围可能会导致信息丢失。

- 修改网络结构，添加残差部分。
  
- 如果采用针对rnn导致的梯度弥散问题，可以使用GRU或者LSTM来解决。
  - 他们存在记忆体，通过遗忘门等门控结构，结合长期记忆和短期记忆，决定哪些记忆需要保留哪些需要丢去，从而保存重要内容。因此不容易梯度消失。
  - 此外也可以通过残差的角度来解释：lstm和gru都有旁路结构，类似于残差33。
  
- 选择合适的优化器，adam，
  - 通过动量和自适应学习率，缓解梯度弥散
    - 动量
    - 自适应学习率

### 梯度爆炸解决方法
- 其实上述的都可以帮助解决梯度爆炸
- 正则化技术：
  - 如L1正则化（损失函数中添加权重绝对值的和，促使模型学习稀疏的权重矩阵）和L2正则化（损失函数中添加权重平方的和，促使模型学习较小的权重值）
  - 通过在损失函数中添加一个惩罚项来约束模型的权重大小，使得权重不会变得过大。这可以间接帮助控制梯度的大小，从而减少梯度爆炸的风险
- 梯度截断：
  - 通过限制梯度的大小，确保他不会变得太大
- 权重衰减，一种正则化技术

# 大模型设计

## 注意力设计

### attention机制是在做什么？以seq2seq来说
每个时间步计算这个时间步与输入数据的相关性，以此求得当前时间步的输出内容和哪几个输入内容最相关，以此辅助生成结果。

没有注意力之前，每次都是根据encoder的输出语义编码c来进行结果生成。而attention帮助模型查看对于当前时间步所需要生成的部分和encoder输入的那些单词，哪几个关系比较大，关系大的就多参考其内容，关系少的就少参考。

又叫交叉注意力机制
- 比较general，因为查询的内容和被查的内容不同，这种注意力机制常用于信息的融合
- 具体操作：
  - encoder输入单词向量作为key
  - decoder当前时间步的影藏状态作为query（当前时间步的输入和语义编码c作为影藏状态）
  - 做query操作，query部分和key进行内积求相关性，得到attention weight
  - 使用softmax进行weight的整合，变为概率分布score作为输出
  - value就是decoder当前时间步的影藏状态
  - 我们将value和score内积，得到最后的对于当前时间步的注意力向量，并输出

### 为什么需要Scaled Dot-Product Attention，也就是为什么需要进行缩放操作（qk内积除以根号dk）？
缩放点积注意力Scaled Dot-Product Attention（这是我们在大模型中用的attention，不过是self attention的版本）：
在进行qk运算后得到attention weight时，还会进行下列操作：
  - attention weight进行除根号dk操作
    - 目的是缩小范围，防止梯度弥散
      - 从数学的角度：当key向量维度较大，会导致attention weight结果会很大，**这将导致 softmax 函数的输出概率分布中某个元素的概率接近1，而其他元素的概率接近0**。这样，对于非1的元素，其梯度（即概率的对数导数）将接近0，这会导致权重更新非常小，从而使得学习过程变得非常缓慢，即梯度弥散问题。
- 这个根号dk的dk值一般是Q矩阵的列，或者K矩阵的行

### 什么是自注意力机制？
- 在attention的基础上改变策略，qkv来源于同一个数据而不是不同的数据。也就是不想attention一样key是外部的，self-attention是聚焦于内部的，qkv都是来自同一数据
- 大白话来说：
  - 序列中的每个元素（如一个单词或者一个时间点上的特征）都会与其他所有元素计算注意力得分（这里的元素都是来自于同一个数据的），这些得分反映了彼此之间的相关性或重要性。然后，每个元素会根据这些注意力得分对其他元素的表示进行加权求和，以生成自己的上下文表示。

### 为什么需要自注意力机制？
- 原有的attention机制只能顺序计算，无法并行计算：
- 因为原有attention的实现，是借助于RNN、LSTM这样的循环神经网络，由于循环网络固有的缺陷，后面的计算依赖前面的输出。这导致当数据量大时，需要耗费很长的时间。
- 无法解决长依赖问题：对于序列中距离较远的词语很难学习他们的关系，因为是基于rnn这种网络的。

### 自注意力和注意力机制的区别？
- 注意力机制的查询和键是不同来源的
  - 例如，在Encoder-Decoder模型中，键是Encoder中的元素，而查询是Decoder中的元素。在中译英模型中，查询是中文单词特征，而键则是英文单词特征。
  - 自注意力机制的查询和键则都是来自于同一组的元素
    - 例如，在Encoder-Decoder模型中，查询和键都是Encoder中的元素，即查询和键都是中文特征，相互之间做注意力汇聚。
    - 也可以理解为同一句话中的词元或者同一张图像中不同的patch，这都是一组元素内部相互做注意力机制
    - 因此，自注意力机制（self-attention）也被称为内部注意力机制（intre-attention）。
- 自注意力并行化处理能力
  - 因为自注意力完全不依赖上一个时间步，qkv都是来源于同一组数据

### 多头注意力机制是什么？
从数据结构上说其实就是在注意力前面增加了一个批次的维度。
- 也就是将输入序列投影到多个不同的子空间中，每个子空间独立地计算注意力权重，然后再将这些子空间的结果合并。
- 这种机制使得模型能够从多个不同的角度捕捉输入序列中的信息，从而提高模型的性能。

### 什么是MQA (Multi-Query Attention)和GQA (Grouped Query Attention)和mha的区别？
Multi-head attention：
- 每个头都有对应的完整qkv，相对别的精度比较高
- 但是如果遇到长文本任务，显存需求会很高

Grouped-Query Attention:
- 多个头（多个Q）对应一个K和V（被查询的对象），这里可以理解成多个q虽然是不同的问题，但是都去同一个数据库（kV）查询     
- K和V少一些，意味着显存占用少些
- 精度比multi-head低一些，但是可以接受

Multi-Query Attention:
- 精度最差
- 全部query，对应一个kv对

## 位置编码

### 为什么需要位置编码？
需要序列处理能力（自注意力和交叉注意力结果都是无序的）
- 因为没有位置编码，会导致网络无法理解序列数据位置间的关系（也就是attention结果无法根据数据位置变化而发生变化）：
假设情况：      
- 比如我们有句话“我/明天/要/从/上海/去/杭州/。”       
- 这句话传到网络中先将其编码为向量，经过注意力层后出来一样形状的向量。    
- 这个过程中包含了自注意力的过程，因为做attention时需要自己和自己还需要自己和周围context进行attention。
  
但是这样的做法存在一些问题：        
- 将上海和杭州交换，最后由于是求和得出attention，那么这个即便颠倒了他的attention没啥变化，但是颠倒后的意义不同了。        
- 因此这意味着没有序列处理能力。因此自注意力和交叉注意力结果都是无序的。

### 位置编码有哪些？
绝对位置编码：
- 信息叠加在词本身上，第一个位置的向量的位置编码就是1，第二个向量位置编码就是2
- 因此推理的时候有问题：训练的时候编码最多到1000，然后使用时输入序列长度到1001，他就不知道这多出来得一个是很么。可以这么理解，对于模型来说，他只知道世界的长度是1000。
  - 对于绝对位置编码问题的解决方案：
  - 稠密编码，通过训练构建一个位置向量数据库：
    - 此外，为什么不用连续性数据去表达的原因是：
      - 比如1000长度的序列，编码完后，网络中数据都很大，不符合正态分布
      - 此外数据间有大小关系，使我们不愿意见到的

相对位置编码：
- 位置根据比较内容的和自身的距离不同而变化   
- 作用在QK上而不是词本身（qkv）上，叠加在query操作中
- 解决绝对位置编码法位置绝对的诟病。因为可以专注于看到长文本中某一个句子。即便文本长度非常长，但是对于相对位置编码，从单句来看，他自己相关的位置依旧是原来那样。

旋转位置编码：容易考

### 什么是旋转位置编码？
旋转位置编码就是我们常说的RoPE
- 首先我们在单位圆上，把位置分成n等分，n = 未来你能够支持的最大的长度来决定，也就是S（这个最大长度是暂定的，超过这个长度模型会自己扩展）。     
- 然后这个圆上，每个单位向量就是位置向量，然后我们算出的q和k向量分别乘上位置向量。

为什么用乘：
  - 因为计算的时候是q乘q的位置 * k乘k的位置。用交换律的感觉。     
  - 其实可以等价于q* k * q的位置* k的位置。Q和K位置是单位向量，那么位置相乘就相当于求内积，单位向量求内积就是等于求余弦相似度，也就是说位置的距离就是余弦相似度。      

## 为什么需要因果注意力矩阵？
对于推理任务，我们的attention是无法处理的，因为attention在关注当前数据的时候也会关注到未来的数据，比如“我爱你”，在训练的时候其实attention关注到了这句句子的全部内容，从而使得训练效果较差，loss直接降到0左右。相当于在训练的时候告诉你真实值是什么。（也就是transformer透题的问题）

为此，通过掩码的方式，让模型看起来像一个因果模型。掩码为上三角矩阵，设置为负无穷，将这个矩阵*原始矩阵，得到一个masked QK

好处：
- 容易并行运算：因为矩阵是一口气算出来的，而不是rnn（自回归模型）那种有了上次才能推下次
- 增加了因果能力
- 解决attention透题问题

## 什么是transformer？
encoder和decoder构成：
- encoder部分：
  - 输入文本的embedding
  - 位置编码
  - 多头注意力机制
  - RMS normalization
  - 残差
  - FFN
  - RMS normalization
  - 残差
  - 输出至decoder的多头注意力
- decoder部分：
  - 上一个时间步的token embedding输出
  - 位置编码
  - 因果掩码的多头注意力
  - RMS normalization
  - 残差
  - encoder的输出
  - 多头注意力机制
  - RMS normalization
  - 残差
  - FFN
  - RMS normalization
  - 残差
  - 线性层输出
- 最后一层decoder后：
  - embedding映射回原来的维度，作为ids输出

## 什么是大语言模型
说的越全越好！！！

大模型是：
- 
大模型有哪些应用：
-
这些应用对应的行业龙头企业和大模型agent产品：
-

- 可以先简要说下transformer的行业应用。
- 优点缺点和我的理解：
  - 全局数据搜索能力（attention）
  - 问题怎么进行解决的。
- 自上而下介绍技术细节：
  - 模型结构：
    - encoder，decoder
    - 因果注意力
    - 细节和他的亮点：
      - 无论编码解码，很多layer，每一层包含啥
      - 怎么实现的
      - gqa（节约现存）
      - 前溃层，喜欢门结构

- 结尾说自己找了数据，从头到尾设计，训练，了0.5B大模型，参考llama2源码进行构建，效果还是不错的

- 然后肯定会问你，参数怎么选！
- 用了多长时间，用了多少数据，用了什么卡

## FFN前馈层
FFN结构：
- 右边这个Linear + SiLU相当于一个门结构，控制信息流量，用于留下主要信息。     
- 这里左边的两个linear是设计的瓶颈结构/梨型结构，用于留下主要信息/获取更多特征。        
- 还有个残差

## Moe专家FFN层
让多个专家模型来完成任务，每个专家可能擅长的领域不同。
- Moe有多个专家（FFN）构成
- 不需要自己制定哪些专家学习哪些数据，大模型训练的时候这个是黑盒

作用：增加参数量的情况下减少计算量，所以都是硬路由
- 硬路由：设置的固定个数的专家（常用
- 软路由：所有专家都有作用，输出概率分布（专家一起回答然后再整合信息

## 网络层normalization
为什么需要normal？
- 因为如果在输入的数值分布过于极端，容易导致梯度弥散或者爆炸
  - 因此需要控制输入值的分布（控制在均值0，方差1的分布范围）
    - 不过也可以不是这个正态分布
- 通过normal去同存异，以此让网络更快的收敛

## RmsNormalization
一种layerNormalization。如果数据是NSV结构，那么RmsNorm就是对sv（每句话中所有token）进行norm
- 计算公式：基于batch normalization，把减去均值操作去除，然后把batch norm中的平滑操作Beta去除保留r。
- 主要区别就是减不减均值
  - rms保证了变化的比例，看到不同点

## 各种normalization（batch，laye，instance，group） 的区别是什么
区别是他们normalization 的目标不同。

首先从本质出发，normalization主要的目的是进行数据分布的矫正，将数据分布变为标准正态分布，也就是均值为0，方差为1.目的是解决我们神经网络的梯度弥散爆炸问题。但是除此之外，根据normalization的目标不同，我们可以实现不同的功能。

对于batch normaliztion：
- 假设输入数据结构是NSV（时间序列数据），也就是n批次，s序列长度，v个序列的特征数。
- 我们是对一个批次中，每个序列，同一个序列位置（token）的同一个位置的标量进行normal。
- 这个方式适合用于图像，这是因为在图像中，结构为n批次，c通道数（特征图数），h特征图高w特征图宽，我们是对一个批次的每个特征图的每个空间位置进行normal。
- 由此可见，对于时间序列数据的处理，如果使用batch normalization，意味着是夸时间序列的对比，这个是不合理的
  - 例如：
    - 处理文本数据，我们在意的是一句话中词的关系，但是如果做batch相当于是在对所有句子的每个词的这个词的特诊进行normal，那么就是在比对不同句子不同词的信息，因此不合适用batch处理序列数据。
- 但是对于图像数据本就是关注图片见像素点的关系，因此batch适合图片。
- 用于：
  - 对一堆数据进行分类（多样本分类），不同数据间特征的不同

对于Layer normalization：
- 假设输入数据结构是NSV（时间序列数据），也就是n批次，s序列长度，v每个序列的特征数。
- 我们是对一批次中一序列中一个token进行normalize。
- 举个例子：
  - “今天在北京”，“今天在上海”，这里的“今天”是一个token
  - 我们希望同样的token在不同句子中这个token本身的特征是不变的，所以layer是对单个token做normal

对于Instance normaliaztion：
- 对于序列数据，就是对单个批次，也就是一句话中的所有token进行归一化。但是当然，也可以对这所有token中同一个位置的特诊进行normal。
- instance norm常用于图像：对一个图片的某个通道的所有值进行norm，这个通道在模型中最开始可能是rgb，不过到后面特征提取后就是常说的特征图。
  - 注意：这里的通道可以是图形通道，也可以是声音通道
  - 因此，当对通道进行norm后，可以统一调整这个通道的数据，用来去掉图像的风格，也可以通过逆向来控制风格迁移。


为什么transformer用layer这类的normal形式而不是batch？
- 因为batch会破坏单个样本中token的序列关系

## 超球面normalization？


# 模型优化

## 瓶颈结构目的
- 减少计算量
- 去除杂质，保留主要特征

## 瓶颈结构为什么减少了计算量
假设没有瓶颈为：
（1，1000）@（1000，1000）=（1，1000）
- 计算量：1*1000*1000 = 1 * 10的6次方
假设瓶颈为：
（1，1000）@（1000，100）= （1，100）
（1，100）@（100，1000）=（1，1000）
- 计算量：1*1000*100 + 1*100*1000 = 1*10的5次方 + 1*10的5次方

## 为什么需要初始化权重？
- 目的是控制权重的极端值
- 常用xavier均匀初始化
- 对于激活函数的初始化，常用kamming初始化

## 如何优化大模型训练和推理，加速训练和推理？
训练：参考下面，从加速收敛，减少显存占用的角度入手

推理：
- kv-cache
  - 节约显存，减少计算量，因此加速推理
- 量化
  - 量化为int型，更好的利用了硬件的性能，推理速度比浮点数型快很多
  - 减少显存占用，减少计算量，因此加速推理
  - 因为占用少了，所以每次信息传输的时候所需的内存带宽减少了，从而提高了信息传输效率，增加了推理速度
  - 量化的别的方法：模型剪枝，知识蒸馏，减少显存占用，计算量，提升模型推理速度

- flashattention
  - 加速注意力计算：sram中小批次的数据进行运算，而不是一次性算较大的数据量而导致sram内存不够需要反复将数据存入gpu再拿到sram算。
- deepspeed推理（并行计算）/别的推理框架
  - 如果是自己的模型进行推理就用deepspeed
  - 不然用现有的推理框架也挺好的：
    - VLLM，OLLama，Dify
    - 他们提高模型推理的方法不同
      - vllm：并行计算
      - ollama：优化计算图和内存管理
      - dify：轻量级，占用资源少

## 如何监控和优化大模型的训练效率和稳定性？
监控：tensorboard监控loss，梯度，权重
优化：
- 训练效率
  - 加速模型收敛：
    - 分布式训练（数据并行）
    - 权重初始化（loss起点比较低，不然可能从很高的loss开始下降）
    - 数据预处理，standardization（统一分布）
    - 使用合适的优化器/修改学习策略（先adam，最后采用sgd）
    - flashattention，加快计算速度
    - 优化模型结构，减少计算量（瓶颈结构
  - 减少显存占用（那么占用少了，就可以喂更多的数据，也能加快模型的训练效率
    - zero optimization
    - 梯度累加
    - 冻结网络层
    - Lora
    - QLora：量化基座模型
    - GQA分组注意力
    - 混合精度运算
    - Flash Attention
    - 优化器：选梯度下降，比adam节约显存，可以是在模型最后期的时候进行优化器的更改，让他更好的去收敛
  
    - 基本不用：但是也算节约显存，不过降低了性能
    - offloader：计算结果存在内存，一直需要内存交互所以慢
    - gradient checkpointing：不存信息，每次需要优化器信息和梯度信息的时候，再前向传播求数值，计算数值
- 稳定性
  - 目的让模型训练稳定
    - 对数据进行过度，每个文件都堆叠点前面文件的数据增强后的数据，可以让损失下降稳定点
    - 避免梯度弥散爆炸，提高稳定性
      - 弥散：
        - 激活函数（sigmoid，tanh有饱和区，no！）
        - 网络层数
        - 残差
        - 初始化权重
        - 预处理数据normalization缩放0-1（输入不会太大太小会导致弥散爆炸）
        - 在网络上的normal例如layer，rms，batch
      - 爆炸：
        - 正则化
        - 梯度裁剪
    - 合适的优化器（adam，自适应学习率
      - 学习率策略：warm up最开始以较小学习率训练，然后逐渐增加到预定的学习率的技术
      - 余弦退火：在一个周期的学习过程中，让学习旅从大到小逐渐衰减，确保训练稳定
  - 设立检查点，方便回溯

## 优化器

### 优化器可以有哪些？
常用的就是梯度下降法
以及adam优化器

### 为什么需要adam？
因为光使用梯度下降法，容易陷入局部最优，也就是鞍点问题。

- 因此，当我们使用adam进行网络优化的时候，非常适合用于模型训练的前期
  - 使用动量法和自动调整学习率（自适应学习率）等方式进行较快的模型收敛。
  - adam默认超参数设置的表现比较好，减少手动调参的工作量
- 但是等后期，模型收敛至最优点附近时，常见损失不下去，无法进行进一步收敛，这个时候可以使用梯度下降在慢慢的收敛至最优点。

### adam的缺点？
adam非常消耗显存，消耗的现存是权重的两倍，因为公式里面涉及beta的第二和第三步（一阶和二阶矩阵）都需要使用**梯度**进行计算
- 也就是说：
  - 显存 = 1倍的梯度 + 1倍的权重 + adam（2倍的权重或者梯度）
  - 举例模型参数为1B：
    - 模型显存消耗=1B * 4Bytes/参数=4GB 
    - 优化器消耗=1B * 8Bytes/参数=8GB
    - 梯度显存消耗=1B * 4Bytes/参数=4GB
    - 因此总显存占用为4+8+4+其他，约等于16GB

### 如何优化显存（训练阶段？
- Zero0/1/2/3优化器：(卡越多，效果越好，单卡只能zero0)
  - 常用zero1或2
  - Zero0：每个GPU有相同的权重（参数），优化器状态，梯度。传统的数据并行，数据按批次在各GPU上并行处理
  - Zero1：每个GPU有相同的权重，梯度。传统的数据并行。
    - 优化器状态（beta1，beta2）分布在不同的显卡上。降低显存
  - Zero2：每个GPU有相同的权重。传统的数据并行。优化器状态分片。梯度也分片。降低显存
  - Zero3：极端，上述全部分片在每个GPU上
  - ！！缺点！！：对带宽要求高，因为要频繁数据交换。
- 梯度累加:
  - 通过将一个大批次分成多个小批次，每次只处理一个小批次的数据，将计算得到的梯度累加到模型参数的梯度中而不更新，显存消耗大大降低。
  - 例如，处理128个样本时，如果直接使用128的批量大小，显存消耗为18GB；使用梯度累加，每次只处理32个样本，显存消耗约为4.5GB（4.5GB = 18GB / 4）。
- 冻结部分层:
  - 和kv-cache一个感觉，锁住某些层，也就是这些层的权重固定，我也不需要拿到gpu中去计算，从而节约显存消耗
- QLora：
  - lora意味着以训练ab矩阵
  - Q意味着量化
- flashattention：
  - 加速注意力计算：sram中小批次的数据进行运算，而不是一次性算较大的数据量而导致sram内存不够需要反复将数据存入gpu再拿到sram算。
  - 节约显存：Flash Attention 通过将输入序列分成多个小块，并在每个块内独立计算注意力，减少了每次计算的显存需求。
- GQA分组注意力机制
  - kv对比原本少了一半
  - 显存占用也找了一半
- 训练方式的调整: 比如使用混合精度运算
- 更改优化器:
  - 例如梯度下降算法，就没有adam那么复杂要记录adam状态
- 修改输入数据长度:
  - 也就是给gpu输入进去的需要运算的数据更少
  
- offloader：（一般特别穷的时候用，因为牺牲性能）
  - 在计算时，将前面层的东西放到内存中，从而节约显存
  - 但是问题是计算会比较慢因为需要一直和内存交互，所以需要考虑！！带宽！！问题。
  - deepspeed中的优化器里面，可以使用offload为true
- gradient checkpointing:（一般特别穷的时候用，因为牺牲性能）
  - 不保存部分优化器和梯度信息，在需要梯度的时候前向推理，从而计算出来。
  - 通常选择计算成本较高的层进行 checkpoint，以最大化显存节省。
  - 缺点：性能降低很多，毕竟要重新前向传播一遍。

- 使用svd的思路处理优化器（一种新方法），进行权重的缩放
  - 会降低精度，每次算的时候很麻烦
  - 但是可以节约60%显存


### 如何优化显存（推理阶段？
KV-cache（只用于因果模型）：
  - 推理的时候，随着文本长度越长对现存要求越多，因为额外要存，所以会增加显存
  - 因为在自回归生成任务中（例如文本生成），模型在推理每一个词的时候，都需要对之前的kv数值进行计算（kv的linear进行映射以及RoPE的计算），这样是非常浪费算力和显存的。
  - 因此采用kv-cache去缓存每个时间步的算好的kv，这样在下个时间步的时候提取出上一个时间步的kv与当前时间步的kv合并下，就可以做attention了。
  - 此外，降低了时间复杂度：
    - 因为使用kv-cache只进行当前时间步的注意力计算，复杂度为On，但是非KV-cache，又需要计算kv又需要计算注意力，因此是O(n**2)
- tips：如果是窗口注意力，那么就是O1

量化：
- int4，nf4
- 剪枝，知识蒸馏



## 如何加速模型收敛？
在train阶段预测的时候，预测结果-它的mean，可以加速收敛。
- 分布式训练（deepspeed，由optimization决定，咱们选用的是zero optimization
    - 数据并行
- 初始化权重：目的是控制权重的极端值
    - 如果初始化为0会有啥问题？
        - 那么网络无论输入什么数据，他们每个神经元输出的权重都是一样的，虽然梯度会更新，但是每次更新也是一样的，这没有意义，网络根本无法学习特征。
        - 权重初始化为0使得，每个激活单元都是同一个值，也就是所有激活单元计算相同的数值，网络变得跟只有一个隐含层 节点一样
    - 此外，预训练那个项目，当初词表30000，模型应该从loss10开始下降，但是从20下降，因为没有初始化权重。因此可以看到，做合理的权重初始化，可以帮助模型更快收敛
- 数据预处理：standardization：
    - 将数据缩放到统一标尺（分布
    - 原因：
      - 首先，因为我们常用的优化器是adam，他是基于梯度下降算法的。然而梯度下降算法对于数据尺度较为铭感。
      - 当数据的尺度不一致时，优化器可能会在不同的方向上表现出不同的收敛速度。通过标准化，可以确保不同特征之间的尺度相似，从而使优化器更容易找到最优解，以此加快收敛速度。
- 选择好的优化器，adam，可调整学习率
- flashattention 加速注意力计算，因为sram那的操作。减少显存需求，因为有分块注意力
- 优化模型结构，减少计算量（瓶颈结构

### 学习率除以10和loss除以10，可以等价吗？
##############################################################################################################################################################

### 如何调整学习率，如何选择学习策略？
以下调整学习率方法：
- 基于deepspeed
  - 使用lr-schedule：
    - warm up：最开始以较小学习率训练，然后逐渐增加到预定的学习率的技术
    - 余弦退火：在一个周期的学习过程中，让学习旅从大到小逐渐衰减，确保训练稳定
  - 使用adam：
    - 自适应学习旅

以下选择学习策略：
- 选择优化器：
  - 先adam
  - 后sgd
- 混合精度训练：
  - FP32: normal层，损失计算，权重更行
  - FP16: 其他层


## 量化
### 什么是量化？
把FLoat类型（FP32，FP16）的模型参数和激活值，用整形（int8，int4）来替代，同时尽可能减少精度损失。
- 减少显存占用
- 减少传输数据量，加快推理
- 整形运算快比浮点数，加速推理

### 部署前优化
- 量化：
  - 量化极端值问题：
    - 只要是线性量化都有极端值问题：
      - 极端值（其实不应该被优化掉，因为极端值往往是数据的主要特征
      - 出现极端值导致中间的空间存在大量的浪费，而正常分布的数值很多被量化为同一个整数
      - 就像本来是fp32表达的数据你用int4去表达，那么可能int4中一个位置需要表达fp32中可能10个位置的数据，
      - 那么在这个时候如果大多数fp32数据集中在一起，他们很容易被一个数去表达。
      - 这就造成了严重的精度损失。
  - 按方式分类
    - tips：前两个
    - 动态量化：
      - fp32输入，量化为int8，然后再输出的时候再反量化
      - 优点：不需要样本提前计算，因为可以直接用
      - 缺点：边推理边量化，因此慢
    - 静态量化：
      - 提前计算量化的scale，offset，需要用样本进行提前估算
      - 这样就不用边推理边计算量化参数
      - 优点：推理的时候不用算，因此推理很快
      - 缺点：训练好后还要用训练集去算参数
    - 感知量化：
      - 量化完了要进行微调，弥补量化后带来的精度损失
      - 优点：
        - 算是线性量化，可以发挥gpu的性能，推理速度块
        - 通过微调弥补精度损失
  
  - 按计算方式分类
    - 对称量化：线性量化
      - 将数值范围的缩放到-127~127
      - 缺点：对极端值处理不了，量化精度低
      - 优点：充分利用硬件整型能力，计算很快提高性能
  
    - 非对称量化：线性量化
      - 将数值范围缩放到0~255
      - 缺点：数据分布集中再某个部分，依旧导致了量化精度低
      - 优点：解决对称量化的缺点。
        - 充分利用硬件整型能力，计算很快提高性能
  
    - normal量化（非线性的，nf4/norm 4byte）：
      - 为什么要用normal量化？
        - 首先可以把nf4理解为是符合正态分布的int4类型的一个标尺，然后呢我们权重的分布是正态分布的。那么因为他们都是正态分布的情况，因此可以量化出来的精度损失比较少。但是打个比方，如果我们权重是均匀分布的，那么使用这个正态分布的标尺去进行量化，精度损失会很大。
  
      - 缺点：非线性量化，推理性能低，不能充分利用硬件整型能力
        - 此外，因为是非线性的，所以量化后无法反量化
      - 优点：非常节约现存，不损失精度
  
    - tips：还有个叫做双重量化
  - 相关算法：
    - GPTQ：感知量化
    - AWQ
  - 现在还有个微软的bitnet 1.58，量化为三进制，可以在cpu上跑出gpu的性能

- 剪纸：
  - 把作用小的权重和权重块去掉
  - 按照权重去去除（常用方法
  - 按照通道去去除
    - 提高模型的性能，要看硬件支持不支持
  - 剪纸后，权重数量会变化，但是能不能提高性能主要看硬件支持不支持

- 蒸馏：
  - 通过**标签（结论）**去教导小模型，还需要从**结构**上教导
  - teacher-student模式：
    - 大模型出来的是t-softmax，也就是大模型输出的概率分布，将对应分布的概率当作学生模型的标签，去训练小模型
    - 所以最开始t设置大点，让曲线比较平坦，随着训练的加深，让t越来越小，这个方式叫做模拟退火。
    
    - 此外不仅通过**标签（结论）**去教导小模型，还需要从**结构**上教导，因此：
      - 此外还有个方法，对模型层与层做损失，使用均方差
  - self模式：
    - 此外还有自蒸馏，可以让模型训练稳定，泛化能力提升


## 残差与门控
旁路上如果是控制信息流量（控制输出层的激活，让重要信息流过，过滤不重要信息），那就是门，如果是放所有信息，那就是残差。
- 残差目的：增加信息的流动量，防止梯度弥散
- 为什么可以防止梯度弥散？
  - 反向传播的时候除了经过原始哪些路径，还可以经过旁路，旁路上的信息和原始路径的信息堆叠，使得信息更加顺畅的流通
  - 形象来说：权重类似于一个阻力，会阻止信息的流动，加了残差旁路后，使得信息流动更加顺畅，从而防止梯度弥散


门结构：
哪些网络用到门结构：LSTM，GRU，FFN，IA3

门可以用什么实现？
- sigmoid（激活过滤）
- tanh（激活过滤）
- silu/swish（不仅激活过滤，还能放大一些重要信息）

残差要求，输入输出同形状的，不同形状那就加个映射。（门结构可以无脑加，效果都会好些）

## dropout
训练过程中随机断开一些神经元的连接，丢弃权重。防止过拟合。

添加在网络的激活前网络后，添加dropout（因为在这里，normal算出来了还是均值0，方差1，不然就会改变输出的分布变化）

dropblock：按照块的方式去丢弃，用于图像

# 大模型评估

## 测评用数据集有哪些？
常见的测试数据集：（每一种数据集起码需要记住2个，应对面试
- 数据集本来就小，基本全部都要用到
  
- 知识与语言理解方面：
  - mmlu（英文
  - glue
  - sciq
- 推理能力方面：
  - gsm8k（英文
  - crass
- 数学评估
  - MATH
- 中文数据集：
  - c-eval
- https://blog.csdn.net/weixin_43431218/article/details/135631534
- https://blog.csdn.net/qq_36803941/article/details/140045494

## 怎么评测大模型？
先定义大模型新名词
- 对于有预训练后的模型，叫做基座模型
- 对于微调后的模型，叫做对话模型

多维度评估：
- 通用能力
- 学科综合能力
- 知识能力
- 推理能力
- 理解能力
- 安全能力

评估方法时：
- 客观评测
  - 做选择题直接输出答案这种很好评测，相当于分类任务的评测。
  - 但如果式输出一句话，那么就得计算困惑度，从而将其转换为分类问题
    - 主要看困惑度（涉及到求整句话得联合概率），现在大多数也看精度accuracy
    - 对于困惑度：
      - 困惑度越小越好，因为里面是商，商越小越好
      - 困惑度超过一个阈值，认为是错的，反之是对的
- 基于llm的主观评测
- 基于人类的主观评测

我们知道除了分类任务，常用得任务还有回归任务。
- 对于分类任务进行客观评测：
  - 使用距离来衡量：
    - MAE/MSE
    - 缺点：没有衡量上限，但是我们期望的指标是有0-100分，有上限的。
      - 因此虽然这两个指标一直在用，但是不满足我们的要求，这个虽然简单好用在同一类还ok但是跨物种就不太好，不规范
  - 精度accuracy衡量（针对全部样本）：
    - TP+TN / TP+TN+FP+FN
    - 数据不均衡时，无法算出质量
    - 思南评测都是精度
  - 准度：
    - 查准率Precision
      - 只看正样本，在实际为正样本，判断正确的概率
      - 用于看有没有把正确的样本全部预测为正确
      - TP / TP+FP
    - 查全率Recall
      - TP / TP+FN
      - 预测是正，并且真的是正的概率
      - 用于看预测为正的样本有多少真的是正
  - 基于准度的P-R曲线与F1-Score：
    - P-R曲线：
      - 图像上看就是，找所有由Pre和Recall构成的曲线中，哪个曲线的平衡点做靠右。
      - 这意味着对比其他，这个曲线在查准和查全方面都达到了最佳平衡点。
    - F1-score：
      - 0~1之间，越靠近1越好，越0越不好
      - 分类指标中最重要的指标
    - ROC曲线与AUC值：
      - ROC：
        - 由假正率（FPR）和真正率（TPR命中率又称recall查全率/召回率）构成
        - TPR：TP / TP+FN，代表所有预测为正的样本中，预测正确的概率
        - FPR: FP / TN+FP，代表所有预测为负的样本中，预测正确的概率
      - AUC（ROC下的面积）：
        - ROC曲线下面面积越大，说明模型效果越好
- 回归任务客观评测：
  - R2-score：
    - R2越靠近1越完美，越靠近0越差

## 模型测试验证（训练前测试，看看模型有没有设计失败
- 形状测试
  - 模型参数形状等查看是否正确，边写边测试，看看是否是自己想要的
- 过拟合调参
  - 使用极小数据量，让大模型快速收敛
    - 如果模型收敛很快但是最后没收敛到0左右，那么是模型有问题的
    - 虽然模型收敛到0左右了，但是收敛曲线没有弧线的很快下降，或者降低到下面后有反弹，那么是模型有问题的
  - 但不是绝对的！
  - 过拟合调参的时候，收敛快，且loss收敛到0，并不一定代表了模型训练没问题！不过过拟合测试不通过一定有问题！！！
- 因此，通过了过拟合测试，任然需要保持怀疑态度，毕竟这个时候再有问题都是在可以找的范围内！

- 能够发现的最常见问题：
  - 梯度爆炸弥散
  - 是否在想要的loss下降

## 预估模型损失下降起点：
条件：词表30000，xavier初始化权重       
答：从loss10下降
- 在 NLP 任务中，词表的大小（如 30000）意味着模型需要从大量可能的输出中选择一个。初始损失反映了模型在这个随机状态下的预测不确定性。具体来说，当模型随机初始化时，预测每个词的概率接近1/30000，导致每个词的交叉熵损失约为：(以e为底)   
- L≈−log(1/30000)≈log(30000)≈10.3   

# 大模型参数

## 权重衰减是多少？
weight_decay 的值为 0.01，表示权重衰减的系数为0.01。
- 权重衰减是一种正则化技术，用于防止模型过拟合，通过在损失函数中添加一个正则化项来惩罚大的权重值

# transformer的计算量粗略估计
算有权重的部分：
- attention：qkv和输出部分4个线性层，非瓶颈结构 -> 4 * 输入维度 * 输出维度
- ffn：3层线性层，瓶颈结构 -> 3 * 输入维度 * 隐藏层维度

# 开发 0.5B 模型层数怎么设置? (根据参数量来看，参数量根据网络中包含的linear层来看) 
0.5B - 30000 * 2048 / ((2048 * 1536 * 3) + (2048 * 1536 * 2 + 2048 * 768 * 2) [这是考虑embedding 的计算]
0.5B / ((2048 * 1536 * 3) + (2048 * 1536 * 2 + 2048 * 768 * 2) [这是不考虑embedding 的计算]

tops：我们所常说的网络规模比如0.5B都是不考虑embedding中的linear层参数的

0.5b / ((2048 * 1536 * 3) + (2048 * 1536 * 2 + 2048 * 768 * 2)
- 这里(2048 * 1536 * 3)表示ffn层的linear。 
- (2048 * 1536 * 2）表示了attention层Q_head_linear和output_linear，1536是因为在attention和ffn都采用了瓶颈结构。
- （2048 * 768 * 2）表示了attention层中的K_head_linear和V_head_linear的参数量。

30000 * 2048，意味着：embedding部分
- 30000：veco_size
- 2048: output_dim
- 
Tips：
- 值得注意的是，《如果》考虑了embedding部分但是为什么不考虑output_layer部分呢？
  - 那里不是也有一个输入size为2048，输出size为30000的linear layer吗，那么这里不应该是2个30000 * 2048吗？
    - 但是！！！！由于我们进行了权重共享的操作 -> 意味着参数共享，那么output layer不会添加新的参数量。因此只有一个30000 * 2048
    - 



# 硬件相关

## fp16和bf16的区别是什么？
fp16:
- 格式：1位符号位，！！5位指数位！！，10位尾数位
- 显存占用：相比 fp32，fp16 可以减少一半的内存占用
- 训练不稳定：由于 fp16 的动态范围（指数位）较小，容易出现数值溢出和下溢
bf16:
- 格式：1位符号位，！！8位指数位！！，7位尾数位
- 动态范围大：bf16 的动态范围（指数位）比 fp16 更大，减少了数值溢出和下溢的风险

## 混合精度训练
- 性能差距10倍以上
- 显卡必须支持fp16，意味着显卡一定要有tensor核
工作方式：模型的权重和计算大部分使用FP16进行，但某些关键部分（如累积梯度、权重更新等）使用FP32。

## 分布式训练

### 数据并行和模型并行的区别是什么？
数据并行：
- 是指将数据集分成多个子集，每个子集由不同的工作节点（worker）处理。每个工作节点拥有完整的模型副本，并在各自的子集上独立进行前向和反向传播。然后，将各个节点的梯度汇总，更新模型参数。

模型并行：
- 是指将模型的不同部（不同的网络层）分分配到不同的工作节点上，每个节点只负责模型的一部分。

### 他们的优缺点是什么？
对于数据并行：
- 优点：随着数据集的增大，可以通过增加更多的工作节点来提高训练速度。
- 缺点：显存占用高，每个节点都需要存储完整的模型

对于模型并行：
- 优点：每个节点只需要存储模型的一部分，减少了内存需求
- 缺点：实现比较复杂，最后要把他们全部整合起来

- tips：他们共有的缺点是，对带宽要求高
  - 梯度同步需要在节点之间进行通信，当节点数量增加时，通信开销会增加。

# 模型训练
模型训练三阶段：预训练，指令遵循训练（SFT），对齐训练（RLHF）

## 模型微调
微调一般训练轮次较少3-6轮

### 什么是SFT？
就是让模型根据指令进行回答，主要对数据进行处理：prompt+回答的结果 过程sft数据。
- 一般是问答数据，指令相关的数据，数据量一般不大

和预训练区别？
- 对数据质量要求很高（预训练要知识多

计算过程：
- 使用负对数似然，交叉熵损失的一个特例，它只针对单个样本点进行计算

指令是按照有一定的格式chat template，例如Qwen：
- system + user + assistant，具体可以网上查一下

### COT思维链也是属于sft阶段的训练，如何实现？
思维链就是让模型按照更加细分的步骤进行问题的回答：    

1. 因此cot可以再prompt中：
在system-prompt中，以下是一个示例：
**
  问题：计算 15 × 12。
  推理步骤：
  1. 将 15 分解为 10 + 5。
  2. 计算 10 × 12 = 120。
  3. 计算 5 × 12 = 60。
  4. 将结果相加：120 + 60 = 180。
  结论：因此，15 × 12 = 180。
**

2. cot也可以在数据中通过微调让模型理解

### DPO怎么实现的？
- 直接偏好优化
- DPO是RLHF的无偏损失函数
- 优点：
  - 只需要chosen和rejected样本，不需要奖励模型
  - 把强化学习变成了监督学习

DPO 是一种自动微调方法，它通过最大化预训练模型在特定任务上的奖励来优化模型参数。

为什么需要DPO？
  DPO绕过了RLHF那样建模奖励函数这一步，而是通过直接在偏好数据上优化模型来提高性能。
  - 这意味着不需要像RLHF那样使用近端策略优化ppo中需要构建奖励模型一样，要对奖励模型进行拟合，采样，和超参数调整。

如何实现？
  - 在实现过程中，DPO使用LLM作为奖励模型。它利用**二元交叉熵作为损失函数来优化策略**，**借助人类偏好数据来确定哪些回应是被偏好的**，哪些则不是。通过将模型的回应与偏好的回应进行比较，以提高其性能。
  - 因此最关键的就是数据集中是包括**chosen的数据（根据问题被选择的回答）以及rejected的数据（根据问题被拒绝的回答）**
  - 从公式的角度来说：
    - 公式：L = -[sigm(beta log(要训练的模型/参考模型) - beta log(要训练的模型/参考模型))]
    - 由于我们期望是将loss最小化:
      - 那么我们期望【】中的内容最大化，也就是第一个log结果最大化，第二个log结果最小化
      - 因此，第一个log中要训练的模型使用的数据是chosen的，第二个log中要训练的数据是rejected

DPO的缺点？
- 选择了动作但是如果概率特别小，这意味着当被log一个很小的数值的时候，结果几乎为0，因此网络就会出错。
- （因为有选择部分和拒绝部分）这种对比学习的时候，其实应该加距离，因为单纯这么设计很容易出现的问题是：
  - 公式前半和后半的结果差不多，以致于差值很小

Solution：      
- IPO，添加了距离的概念，在原有的基础上减去最小距离（可以理解为一种正则化）
  - （也就是好的模型和差的模型起码要有这个常数的数值的差距
- 优点：增加泛化性

### 什么是KTO？
通过对参考模型做KL散度，使得模型更加稳定（可以理解为一种正则化）
- 优点：
  - 不需要像DPO一样收集正负样本
    - 就是同一句话标注为good或者bad。但是原来按照dpo的方法，我们需要寻找成对的表示能接受和不能接受的数据。   
    - 从而降低获取标注样本的成本
  - 不让变化太剧烈，稳定模型


### Lora怎么实现的？
由原始模型的矩阵+旁路矩阵构成（AB矩阵，他们是低秩矩阵），他们大小相同，不过旁路矩阵为了减少计算量采用瓶颈结构构成，分为上B和下A矩阵。
- （瓶颈宽度就是我们的rank，如果想要微调学习到的信息量少一点，那就设置小一点，一般设置为32）
- 在训练时我们的原始矩阵只负责推理，而右边的AB矩阵参与训练。
- 不过需要注意的是，对AB矩阵初始化时，输入部分的A矩阵初始化位正态分布，输出部分的B矩阵进行0初始化。
- 为什么这么初始化？？
    1. 假设AB都是正态分布：
        - AB都为正太，理论可以，因为可以得到训练，但是实际不稳定。因为B不为0的话，在第一次计算出来的结果再和原始旧矩阵叠加的时候，输出的结果和网络原来的结果很大的不同，以至于梯度较大。导致初始训练的时候，网络稳定性差，发生梯度爆炸的情况。
    2. 假设，A正态分布，B=0 （面试常问）
        - 训练初期：由于B初始化为0，这使得，早期训练的时候，lora的旁路对整个模型影响并不大（因为梯度比较小，这意味着不会破坏以前的权重！！），因此训练稳定。
        - 渐进调整：随着训练的进行，低秩矩阵B会逐渐学习到有用的增量，从而逐步调整模型的权重。
            由于A是随机初始化的，每个神经元的输出在前向传播过程中会有不同的值，梯度在反向传播过程中也会不同，从而使得每个权重更新的方式不同。
    3. 假设，A=0，B是正态分布
        - 如果A=0，那么A矩阵输出是没有意义的，因此b矩阵得不到有效训练。（因此A举证不能设置为0
            - 权重全部初始化同一个常量，意味着网络中每个参数的重要程度是一致的。
            - 那么每一层中的所有神经元都会执行相同的计算。这意味着，在前向传播过程中，每个神经元的输出将完全相同；而在反向传播过程中，每个神经元接收到的梯度更新也将完全相同。这导致了神经元之间缺乏多样性，从而无法学习不同的特征。
    4. 如果AB都是初始化成0
        - 权重全部初始化同一个常量，意味着网络中每个参数的重要程度是一致的。
        - 此外，权重初始化为0使得，每个激活单元都是同一个值，也就是所有激活单元计算相同的数值，网络变得跟只有一个隐含层节点一样

### Lora的优点？
1. 灵活性高，适应性广：
- 不同的知识不同的旁路（也就是不同的BA，我们将它看作旁路）
- 因此我们有主权重（左边的原始权重），那么可以接很多分权重来生成不同细分内容。（做到主模型和次模型分开，因此可以分着模型进行训练）（也就是可以下载专门生成人物啊，或者专门生成漫画的啊，有点像挂载的buff，将他们加载在主模型上）

2. 不破坏原始模型权重的基础上进行微调，解决全量微调可能发生的灾难性遗忘问题。

### RLHF四步骤
Rollout：
- 目的：用需要**被对齐的模型**，通过同样问题**生成多个回答**（通过调整tkp那些参数

人或者ai来打分：
  - ai打分就是**对生成的答案打分**（RLAIF
  - 人类打分就是RLHF
  - 打分一般是0~1，方便训练奖励模型
  - 这个打分就是标签
  
Evaluation：
- **训练奖励模型**：
  - 输入是上述的问答与打分分数
  - 让网络逼近这个打分
  - 只要有奖励就可以使用强化学习算法

Optimization：
- 使用PPO2和奖励模型**给出的奖励分数训练对齐模型**
  - 为什么PPO2？
    - 因为PPO2是openai得意之作
    - 当然也可以用别的

### PPO算法是什么？
- ppo是对策略梯度损失函数的优化（这个过程叫做近端策略优化TRPO）。
- 优化点：
  - 通过重要性采样把PG模型从在线模式改为离线模式
  - 对于PPO1：因为使用了重要性采样，所以用正则化的方式（KL散度），限制采样策略和评估策略（训练的网络生成出的）间变化不要太大
  - 对于PPO2：目的也是变化别太大，因为使用了重要性采样，使用裁剪损失的方式，限制采样策略，使得采样策略和评估策略间变化不大

# AI部署

## 。

# RAG
## 数据库分类有哪些？
结构化数据库：
- 只用一种，关系型数据库
  - 存关系型数据（表格数据
    - mysqk（这个最多
    - oracle（现在也不多
    - sqlite（用轻量级的用这个

非结构化数据库：
- 对象数据库
- 向量数据库（知识向量库
  - 存知识，向量查询
  - 最火：milvus
- 键值数据库
  - 存键值对

## RAG怎么工作的？
- 使用milvus向量数据库，
- 先将数据进行分块和堆叠的设置        
- 然后将数据通过embedding（练习的使用的是ollama上的bge模型）进行向量化，并且构建向量数据库
- 在使用的时候通过相似度搜索，搜索出和我们目标信息，相似度最高的几条数据，将这些数据反馈回大模型，作为参考信息，以完成任务。

## GraphRAG和RAG的区别？
GraphRAG和RAG存储的数据类型是不同的
- 前者为使用的是图结构数据
- 后者为文本文档集合

GraphRAG：
- 优点：
  - 可以更好的了解当前节点与其他信息的关系，因为有边和节点构成。
  - 使用图结构数据，能够提供结构化的背景信息
  - 支持多模态数据
- 缺点：
  - 某些行业，构建图谱成本大：
    - 涉及大量的信息整合和构建：
      - 生物医学图谱构建
      - 社交网络图谱构建
- 应用：
  - 关系推理：适用于需要理解**复杂关系的任务**，如药物发现、金融分析等。
  - 多模态数据融合：适用于**多模态数据融合和综合分析**，能够处理多种类型的数据。
  - 知识图谱构建：适用于构建和查询知识图谱，能够**提供结构化**的背景信息和关系推理。

RAG：
- 优点：适用于文本数据源，例如网页，文章等，不需要考虑数据间关系的情况
- 缺点：对于需要结构化信息的任务，如关系推理和知识图谱构建，效果可能不佳
- 应用：
  - **对话系统**：适用于对话系统，能够提供**丰富的上下文信息**以生成自然流畅的对话。
  - 问答系统：适用于基于文本的问答系统，能够快速检索相关文档并生成答案。
  - 文本摘要：适用于生成文本摘要，能够从大量文档中提取关键信息。

# Agent

# langchain用过吗优缺点啥？
不太好用，用的也不多。用来调用模型加链条，加工具加rag，但是用chain构建agent就很复杂。
现在最近出来的都用langgraph。


# 硬件

## 显卡信息

20系列：伏特架构
- 型号：
  - RTX 20XX：RTX 2080Ti
  - Tesla V100：
    - 16G/32G
    - 网上二手卡：
      - 16G一般3.6k~5k左右
      - 32G一般1.6w~3w左右
  - Quardro:
    - RTX 4000 
    - RTX 5000: 48G
    - RTX 6000: 48G
    - RTX 8000: 48G
- 数据类型：
  - 支持FP32，FP16，Int8
  - 不支持FlashAttention，BF16

30系列：安培架构
- 型号：
  - RTX 30XX：
    - RTX3090
      - 24G
  - Tesla AXX：
    - Tesla A100/A800
      - 40G/80G
    - Tesla A40：
      - 48G
    - Tesla A2/A10：
- 数据类型：
  - flashattention
  - 支持FP32，FP16，BF16，Int8，Int4

40系列：Ada架构
- 型号：
  - RTX 40XX：
    - RTX4090
      - 24G
  - LXX：
    - L20/L40（训练卡）
      - 48G
  - RTX X000A：
    - RTX 6000A（5880A是其替代品）
      - 48G
    - RTX 5000A
      - 24G
    - RTX 4000A
- 数据类型：
  - flashattention
  - 支持FP32，FP16，BF16，Int8，Int4

Hopper架构：
- 型号：
  - H100：80G
  - H200：80G
- 数据类型：
  - 支持FP32，FP16，BF16，Int8，Int4，Int2，FP8

Blackwall:
- RTX 50XX
- B100

图灵架构：
- T4
  - 16G







